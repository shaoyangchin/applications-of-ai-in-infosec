{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network Intrusion Detection Using Machine Learning\n",
        "\n",
        "## Overview of Anomaly Detection in Cybersecurity\n",
        "\n",
        "Anomaly detection is a critical technique that identifies data points exhibiting significant deviations from expected patterns. In the realm of cybersecurity, these anomalies often signal malicious activities, unauthorized network intrusions, or various security breaches. Random forests, as ensemble machine learning algorithms, excel at processing complex, high-dimensional datasets and prove particularly effective for detecting these anomalous network patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Random Forest Algorithms\n",
        "\n",
        "A Random Forest represents an ensemble machine learning approach that constructs multiple decision trees and combines their individual predictions. In classification scenarios, each tree contributes a vote for a specific class, with the final prediction determined by majority consensus. For regression tasks, the algorithm calculates the average of all individual tree outputs to produce the final result.\n",
        "\n",
        "The ensemble nature of random forests provides superior generalization compared to single decision trees, effectively reducing overfitting while delivering robust performance across high-dimensional feature spaces.\n",
        "\n",
        "### Core Principles of Random Forest Construction\n",
        "\n",
        "Three fundamental concepts govern the development of a random forest:\n",
        "\n",
        "**Bootstrapping**: The algorithm creates multiple training data subsets through sampling with replacement, where each subset trains an independent decision tree.\n",
        "\n",
        "**Tree Construction**: During each tree's development, only a randomly selected subset of features is considered at every split point, promoting diversity and minimizing correlations between trees.\n",
        "\n",
        "**Voting Mechanism**: After training completion, classification relies on majority voting, while regression employs prediction averaging across all trees.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying Random Forests to Anomaly Detection\n",
        "\n",
        "When implementing random forests for anomaly detection, the model trains exclusively on data representing normal network conditions. Subsequently, new, unseen data points undergo evaluation against this learned baseline of normal behavior. Data points that demonstrate poor fit or generate low-confidence predictions are automatically flagged as potential anomalies.\n",
        "\n",
        "This methodology enables the detection of unusual patterns, making it particularly valuable for identifying suspicious network traffic and potential security threats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to the NSL-KDD Dataset\n",
        "\n",
        "The NSL-KDD dataset represents an enhanced version of the original KDD Cup 1999 dataset, addressing previous limitations by removing redundant entries and correcting imbalanced class distributions. This refined dataset has become a standard benchmark for evaluating the performance of various intrusion detection systems and machine learning models.\n",
        "\n",
        "NSL-KDD provides balanced, labeled instances encompassing both normal and malicious network activities. This comprehensive structure enables practitioners to perform not only binary classification tasks (distinguishing normal from abnormal traffic) but also multi-class detection operations targeting specific attack types. Such versatility makes NSL-KDD an invaluable resource for developing, testing, and validating intrusion detection methodologies.\n",
        "\n",
        "For this lab, we will utilize a modified version of this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Acquisition and Preparation\n",
        "\n",
        "### Step 1: Downloading the Dataset\n",
        "\n",
        "Prior to loading the NSL-KDD dataset, we must retrieve it from the provided source. We can accomplish this by downloading the compressed .zip file using Python's standard libraries and then extracting its contents locally for further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "# URL for the NSL-KDD dataset\n",
        "url = \"https://academy.hackthebox.com/storage/modules/292/KDD_dataset.zip\"\n",
        "\n",
        "# Download the zip file and extract its contents\n",
        "response = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "z.extractall('.')  # Extracts to the current directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Loading the Dataset\n",
        "\n",
        "Properly loading the NSL-KDD dataset is essential before initiating the preprocessing stage. This ensures that the data maintains consistent structure, with each column containing accurate information. Once loaded, the dataset can be inspected for quality, completeness, and potential preprocessing requirements.\n",
        "\n",
        "#### Importing Required Libraries\n",
        "\n",
        "We begin by importing all necessary libraries for our analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this code snippet:\n",
        "\n",
        "- **numpy** and **pandas** handle data loading and cleaning operations\n",
        "- **RandomForestClassifier** provides the machine learning algorithm we will use for anomaly detection\n",
        "- **train_test_split** and various metrics from **sklearn.metrics** support model evaluation and validation processes\n",
        "- **seaborn** and **matplotlib** assist in visualizing data distributions, relationships, and model results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Defining Column Names and File Path\n",
        "\n",
        "The NSL-KDD dataset includes a comprehensive set of predefined features and labels. We must map these features to meaningful column names to facilitate direct manipulation. We define a comprehensive list of column names corresponding to the various observed characteristics of network connections and potential attacks. Additionally, we establish the file_path variable to point to the dataset file, ensuring that pandas can locate and read the data correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the file path to the dataset\n",
        "file_path = r'KDD+.txt'\n",
        "\n",
        "# Define the column names corresponding to the NSL-KDD dataset\n",
        "columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', \n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', \n",
        "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', \n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', \n",
        "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', \n",
        "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
        "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', \n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These column names ensure that each feature and label is properly identified. They encompass:\n",
        "\n",
        "- **Generic network statistics** (e.g., duration, src_bytes, dst_bytes)\n",
        "- **Categorical fields** (protocol_type, service)\n",
        "- **Classification labels** (attack, level) that categorize the type of traffic observed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reading the Dataset into a DataFrame\n",
        "\n",
        "With the file path and column names properly defined, we proceed to load the data into a pandas DataFrame. This provides a structured, tabular representation of the dataset, facilitating inspection, preprocessing, and visualization operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the combined NSL-KDD dataset into a DataFrame\n",
        "df = pd.read_csv(file_path, names=columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By executing this code, we now have a DataFrame `df` containing all the data from the NSL-KDD dataset with the appropriate column headers. The DataFrame is ready for further inspection, cleaning, and preprocessing steps. Before proceeding, we can briefly examine the dataset's structure, check for missing values, and confirm that all features align with their intended data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing and Dataset Preparation\n",
        "\n",
        "## Overview of Data Preprocessing\n",
        "\n",
        "This section focuses on preparing the NSL-KDD dataset for training a random forest anomaly detection model. The primary objective is to transform raw network traffic data into a machine-readable format by establishing classification targets, encoding categorical variables, and selecting relevant numeric features. We will generate both binary and multi-class targets, ensure categorical data compatibility with machine learning algorithms, and preserve numeric metrics essential for detecting abnormal traffic patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Classification Targets\n",
        "\n",
        "### Binary Classification Target\n",
        "\n",
        "The binary classification target serves to identify whether network traffic represents normal or anomalous behavior. We create a new column `attack_flag` in the DataFrame `df` to accomplish this objective. Each data row receives a label of 0 for normal traffic and 1 for any type of attack. This transformation simplifies the initial detection challenge into a fundamental normal-versus-attack classification, providing a foundation for more detailed analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary classification target\n",
        "# Maps normal traffic to 0 and any type of attack to 1\n",
        "df['attack_flag'] = df['attack'].apply(lambda a: 0 if a == 'normal' else 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The value `normal` originates from the dataset structure. Examining the dataset reveals that all traffic is categorized as either normal or represents some form of attack:\n",
        "\n",
        "```\n",
        "0,tcp,ftp_data,SF,491,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0.0,0.0,0.0,0.0,1.0,0.0,0.0,150,25,0.17,0.03,0.17,0.0,0.0,0.0,0.05,0.0,normal,20\n",
        "0,tcp,private,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,123,6,1.0,1.0,0.0,0.0,0.05,0.07,0.0,255,26,0.1,0.05,0.0,0.0,1.0,1.0,0.0,0.0,neptune,19\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Class Classification Target\n",
        "\n",
        "While binary classification provides useful insights, it lacks the granularity needed for comprehensive threat analysis. To address this limitation, we create a multi-class classification target that distinguishes between different categories of attacks. We define specific lists that categorize various attacks into four major groups:\n",
        "\n",
        "- **DoS (Denial of Service) attacks** such as neptune and smurf\n",
        "- **Probe attacks** that scan networks for vulnerabilities, like satan or ipsweep  \n",
        "- **Privilege Escalation attacks** that attempt to gain unauthorized admin-level control, such as buffer_overflow\n",
        "- **Access attacks** that seek to breach system access controls, like guess_passwd\n",
        "\n",
        "A custom function `map_attack` examines the attack type and assigns it an integer value:\n",
        "\n",
        "- 0 for normal traffic\n",
        "- 1 for DoS attacks\n",
        "- 2 for Probe attacks\n",
        "- 3 for Privilege Escalation attacks\n",
        "- 4 for Access attacks\n",
        "\n",
        "This expanded classification framework enables models to learn not only to distinguish between normal and abnormal traffic but also to identify the specific nature of observed attacks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-class classification target categories\n",
        "dos_attacks = ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', \n",
        "               'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm']\n",
        "probe_attacks = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']\n",
        "privilege_attacks = ['buffer_overflow', 'loadmdoule', 'perl', 'ps', \n",
        "                     'rootkit', 'sqlattack', 'xterm']\n",
        "access_attacks = ['ftp_write', 'guess_passwd', 'http_tunnel', 'imap', \n",
        "                  'multihop', 'named', 'phf', 'sendmail', 'snmpgetattack', \n",
        "                  'snmpguess', 'spy', 'warezclient', 'warezmaster', \n",
        "                  'xclock', 'xsnoop']\n",
        "\n",
        "def map_attack(attack):\n",
        "    if attack in dos_attacks:\n",
        "        return 1\n",
        "    elif attack in probe_attacks:\n",
        "        return 2\n",
        "    elif attack in privilege_attacks:\n",
        "        return 3\n",
        "    elif attack in access_attacks:\n",
        "        return 4\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Assign multi-class category to each row\n",
        "df['attack_map'] = df['attack'].apply(map_attack)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering and Encoding\n",
        "\n",
        "### Encoding Categorical Variables\n",
        "\n",
        "Network traffic data frequently includes categorical attributes that are not directly compatible with machine learning algorithms, which typically require numeric inputs. Two critical features in the NSL-KDD dataset are `protocol_type` (e.g., tcp, udp) and `service` (e.g., http, ftp). These features categorize the nature of network interactions but must be converted into numeric format for algorithmic processing.\n",
        "\n",
        "We employ one-hot encoding, implemented through the `get_dummies` function in pandas. This approach generates a binary indicator variable for each category, ensuring that no ordinal relationship is implied between different protocols or services. After encoding, each categorical value is represented by a separate column indicating its presence (1) or absence (0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoding categorical variables\n",
        "features_to_encode = ['protocol_type', 'service']\n",
        "encoded = pd.get_dummies(df[features_to_encode])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selecting Numeric Features\n",
        "\n",
        "Beyond categorical variables, the dataset contains a comprehensive range of numeric features that describe various aspects of network traffic. These include fundamental metrics like duration, src_bytes, and dst_bytes, as well as more specialized features such as serror_rate and dst_host_srv_diff_host_rate, which capture statistical properties of network sessions. By selecting these numeric features, we ensure the model has access to both raw volume data and more nuanced, derived statistics that help distinguish normal from abnormal patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numeric features that capture various statistical properties of the traffic\n",
        "numeric_features = [\n",
        "    'duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', \n",
        "    'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', \n",
        "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', \n",
        "    'num_outbound_cmds', 'count', 'srv_count', 'serror_rate', \n",
        "    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', \n",
        "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', \n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', \n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', \n",
        "    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', \n",
        "    'dst_host_srv_rerror_rate'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the Final Dataset\n",
        "\n",
        "The final step involves combining the one-hot encoded categorical features with the selected numeric features. We merge them into a single DataFrame `train_set` that will serve as the primary input to our machine learning model. We also store the multi-class target variable `attack_map` as `multi_y` for classification tasks. At this stage, the data is in a suitable format for splitting into training, validation, and test sets, and subsequently training the random forest anomaly detection model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine encoded categorical variables and numeric features\n",
        "train_set = encoded.join(df[numeric_features])\n",
        "\n",
        "# Multi-class target variable\n",
        "multi_y = df['attack_map']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Splitting Strategy\n",
        "\n",
        "In the Data Transformation section, we discussed the rationale and methods for splitting data into training, validation, and test sets. We now apply those principles specifically to the NSL-KDD dataset, ensuring that our random forest anomaly detection model is trained, tuned, and evaluated on distinct subsets.\n",
        "\n",
        "### Splitting Data into Training and Test Sets\n",
        "\n",
        "We use `train_test_split` to allocate a portion of the data for testing, ensuring that our final evaluations occur on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets for multi-class classification\n",
        "train_X, test_X, train_y, test_y = train_test_split(train_set, multi_y, test_size=0.2, random_state=1337)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Validation Set from the Training Data\n",
        "\n",
        "We further split the training data to create a validation set. This supports model tuning and hyperparameter optimization without contaminating the final test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Further split the training set into separate training and validation sets\n",
        "multi_train_X, multi_val_X, multi_train_y, multi_val_y = train_test_split(train_X, train_y, test_size=0.3, random_state=1337)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Split Variables Summary\n",
        "\n",
        "After completing the splitting process, we have the following datasets:\n",
        "\n",
        "- **train_X, train_y**: Core training set for model development\n",
        "- **test_X, test_y**: Reserved exclusively for final performance evaluation\n",
        "- **multi_train_X, multi_train_y**: Training subset for fitting the model\n",
        "- **multi_val_X, multi_val_y**: Validation subset for hyperparameter tuning\n",
        "\n",
        "This careful partitioning, applied after the transformations and encodings discussed earlier, ensures that the model development process remains consistent and that the final evaluation is unbiased and reflective of real-world performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
