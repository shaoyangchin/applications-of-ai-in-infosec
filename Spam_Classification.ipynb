{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#The Spam Dataset\n",
        "We'll explore Bayesian spam classification using the SMS Spam Collection dataset, a curated resource tailored for developing and evaluating text-based spam filters. This dataset emerges from the combined efforts of Tiago A. Almeida and Akebo Yamakami at the School of Electrical and Computer Engineering at the University of Campinas in Brazil, and José María Gómez Hidalgo at the R&D Department of Optenet in Spain.\n",
        "\n",
        "Their work, \"Contributions to the Study of SMS Spam Filtering: New Collection and Results,\" presented at the 2011 ACM Symposium on Document Engineering, aimed to address the growing problem of unsolicited mobile phone messages, commonly known as SMS spam. Recognizing that many existing spam filtering resources focused on email rather than text messages, the authors assembled this dataset from multiple sources, including the Grumbletext website, the NUS SMS Corpus, and Caroline Tag’s PhD thesis.\n",
        "\n",
        "The resulting corpus contains 5,574 text messages annotated as either ham (legitimate) or spam (unwanted), making it a great resource for building and testing models that can differentiate meaningful communications from intrusive or deceptive ones. In this context, ham refers to messages from known contacts, subscriptions, or newsletters that hold value for the recipient, while spam represents unsolicited content that typically offers no benefit and may even pose risks to the user."
      ],
      "metadata": {
        "id": "yvmOtxCx153s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading the Dataset\n",
        "The first step in our process is to download this dataset, and we'll do it programmatically in our notebook."
      ],
      "metadata": {
        "id": "xbShJ8q52Cab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_3u3vdzK1u72"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the dataset\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "# Download the dataset\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Download successful\")\n",
        "else:\n",
        "    print(\"Failed to download the dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcxj2FXU1wV9",
        "outputId": "a3fcda2f-f763-4fe4-faec-3f1ee5454f1b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the requests library to send an HTTP GET request to the URL of the dataset. We check the status code of the response to determine if the download was successful (status_code == 200).\n",
        "\n",
        "After downloading the dataset, we need to extract its contents. The dataset is provided in a .zip file format, which we will handle using Python's zipfile and io libraries."
      ],
      "metadata": {
        "id": "6edC0xDG2oLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the dataset\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "    z.extractall(\"sms_spam_collection\")\n",
        "    print(\"Extraction successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrIpITFA2pas",
        "outputId": "d446ca08-5331-4065-ec6a-454aa45055e1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, response.content contains the binary data of the downloaded .zip file. We use io.BytesIO to convert this binary data into a bytes-like object that can be processed by zipfile.ZipFile. The extractall method extracts all files from the archive into a specified directory, in this case, sms_spam_collection.\n",
        "\n",
        "It's useful to verify that the extraction was successful and to see what files were extracted."
      ],
      "metadata": {
        "id": "YRVFjHT82rBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List the extracted files\n",
        "extracted_files = os.listdir(\"sms_spam_collection\")\n",
        "print(\"Extracted files:\", extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaC2_G2q2sgp",
        "outputId": "1e4ce24f-e586-46de-884f-6861d3da3510"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['readme', 'SMSSpamCollection']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The os.listdir function lists all files and directories in the specified path, allowing us to confirm that the SMSSpamCollection file is present."
      ],
      "metadata": {
        "id": "BiMguXeR3JIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the Dataset\n",
        "With the dataset extracted, we can now load it into a pandas DataFrame for further analysis. The SMS Spam Collection dataset is stored in a tab-separated values (TSV) file format, which we specify using the sep parameter in pd.read_csv."
      ],
      "metadata": {
        "id": "UgOQOL4T3LKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\n",
        "    \"sms_spam_collection/SMSSpamCollection\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"label\", \"message\"],\n",
        ")"
      ],
      "metadata": {
        "id": "qwO4J0cY3OMo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we specify that the file is tab-separated (sep=\"\\t\"), and since the file does not contain a header row, we set header=None and provide column names manually using the names parameter.\n",
        "\n",
        "After loading the dataset, it is important to inspect it for basic information, missing values, and duplicates. This helps ensure that the data is clean and ready for analysis."
      ],
      "metadata": {
        "id": "Okqufr6s3RU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"-------------------- HEAD --------------------\")\n",
        "print(df.head())\n",
        "print(\"-------------------- DESCRIBE --------------------\")\n",
        "print(df.describe())\n",
        "print(\"-------------------- INFO --------------------\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_D3EVsV3Qet",
        "outputId": "880c8afd-79d2-472e-eaaf-e26fe60eb733"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- HEAD --------------------\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "-------------------- DESCRIBE --------------------\n",
            "       label                 message\n",
            "count   5572                    5572\n",
            "unique     2                    5169\n",
            "top      ham  Sorry, I'll call later\n",
            "freq    4825                      30\n",
            "-------------------- INFO --------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   label    5572 non-null   object\n",
            " 1   message  5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get an overview of the dataset, we can use the head, describe, and info methods provided by pandas.\n",
        "\n",
        "- df.head() displays the first few rows of the DataFrame, giving us a quick look at the data.\n",
        "- df.describe() provides a statistical summary of the numerical columns in the DataFrame. Although our dataset is primarily text-based, this can still be useful for understanding the distribution of labels.\n",
        "- df.info() gives a concise summary of the DataFrame, including the number of non-null entries and the data types of each column.\n",
        "\n",
        "Checking for missing values is crucial to ensure that our dataset does not contain any incomplete entries."
      ],
      "metadata": {
        "id": "GIVqAJbR3nGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfJCXnfh3s2Z",
        "outputId": "fc796fa4-00fa-49ca-9211-aa8a674cbf0c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            " label      0\n",
            "message    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The isnull method returns a DataFrame of the same shape as the original, with boolean values indicating whether each entry is null. The sum method then counts the number of True values in each column, giving us the total number of missing entries.\n",
        "\n",
        "Duplicate entries can skew the results of our analysis, so it's important to identify and remove them."
      ],
      "metadata": {
        "id": "GUH--gIW3uCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "print(\"Duplicate entries:\", df.duplicated().sum())\n",
        "\n",
        "# Remove duplicates if any\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6ySfFR3vcN",
        "outputId": "7f8f5ddb-297f-47b2-b628-3827c92db250"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate entries: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The duplicated method returns a boolean Series indicating whether each row is a duplicate or not. The sum method counts the number of True values, giving us the total number of duplicate entries. We then use the drop_duplicates method to remove these duplicates from the DataFrame."
      ],
      "metadata": {
        "id": "nmdWfoYn38Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing the Spam Dataset\n",
        "After loading the SMS Spam Collection dataset, the next step is preprocessing the text data. Preprocessing standardizes the text, reduces noise, and extracts meaningful features, all of which improve the performance of the Bayes spam classifier. The steps outlined here rely on the nltk library for tasks such as tokenization, stop word removal, and stemming.\n",
        "\n",
        "Before processing any text, you must download the required NLTK data files. These include punkt for tokenization and stopwords for removing common words that do not contribute to meaning. Ensuring all required resources are available at this stage prevents interruptions during later processing steps."
      ],
      "metadata": {
        "id": "kKXYfr378Z1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the necessary NLTK data files\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "print(\"=== BEFORE ANY PREPROCESSING ===\")\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_uaj--k84yf",
        "outputId": "bf46154f-70ee-4ef9-9dbd-836e07442ee4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE ANY PREPROCESSING ===\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lowercasing the Text\n",
        "Lowercasing the text ensures that the classifier treats words equally, regardless of their original casing. By converting all characters to lowercase, the model considers \"Free\" and \"free\" as the same token, effectively reducing dimensionality and improving consistency."
      ],
      "metadata": {
        "id": "eF4yFfe38-k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all message text to lowercase\n",
        "df[\"message\"] = df[\"message\"].str.lower()\n",
        "print(\"\\n=== AFTER LOWERCASING ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ8sHWUlCQ7U",
        "outputId": "d482e6ec-d514-4d3e-eaa8-284fdaafcf64"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER LOWERCASING ===\n",
            "0    go until jurong point, crazy.. available only ...\n",
            "1                        ok lar... joking wif u oni...\n",
            "2    free entry in 2 a wkly comp to win fa cup fina...\n",
            "3    u dun say so early hor... u c already then say...\n",
            "4    nah i don't think he goes to usf, he lives aro...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step, the dataset contains uniformly cased text, preventing the model from assigning different weights to tokens that differ only by letter case."
      ],
      "metadata": {
        "id": "WdS2jjxsCRm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Punctuation and Numbers\n",
        "Removing unnecessary punctuation and numbers simplifies the dataset by focusing on meaningful words. However, certain symbols such as $ and ! may contain important context in spam messages. For example, \"$\" might indicate a monetary amount, and ! might add emphasis.\n",
        "\n",
        "The code below removes all characters other than lowercase letters, whitespace, dollar signs, or exclamation marks. This balance between cleaning the data and preserving important symbols helps the model concentrate on features relevant to distinguishing spam from ham messages."
      ],
      "metadata": {
        "id": "pww5IoJRCTEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
        "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP9b7WYYCVbQ",
        "outputId": "f61e53ae-f8f9-40f2-a885-b021756beaae"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
            "0    go until jurong point crazy available only in ...\n",
            "1                              ok lar joking wif u oni\n",
            "2    free entry in  a wkly comp to win fa cup final...\n",
            "3          u dun say so early hor u c already then say\n",
            "4    nah i dont think he goes to usf he lives aroun...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step, the text is cleaner, more uniform, and better suited for subsequent preprocessing tasks such as tokenization, stop word removal, or stemming."
      ],
      "metadata": {
        "id": "T0wM7jLHChEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizing the Text\n",
        "Tokenization divides the message text into individual words or tokens, a crucial step before further analysis. By converting unstructured text into a sequence of words, we prepare the data for operations like removing stop words and applying stemming. Each token corresponds to a meaningful unit, allowing downstream processes to operate on smaller, standardized elements rather than entire sentences."
      ],
      "metadata": {
        "id": "fl7_6BGSCiCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split each message into individual tokens\n",
        "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
        "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZXOh58JDS90",
        "outputId": "1b7ba812-0b64-42cd-a138-f45b61d31d1d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER TOKENIZATION ===\n",
            "0    [go, until, jurong, point, crazy, available, o...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
            "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
            "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once tokenized, the dataset contains messages represented as lists of words, ready for additional preprocessing steps that further refine the text data."
      ],
      "metadata": {
        "id": "QDGPeK42DX3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing Stop Words\n",
        "Stop words are common words like and, the, or is that often do not add meaningful context. Removing them reduces noise and focuses the model on the words most likely to help distinguish spam from ham messages. By reducing the number of non-informative tokens, we help the model learn more efficiently."
      ],
      "metadata": {
        "id": "qoGTlJU2DYtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a set of English stop words and remove them from the tokens\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt9FsuycDaQn",
        "outputId": "0ac59c1a-130d-46d2-a46a-45d9b5cdcf30"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER REMOVING STOP WORDS ===\n",
            "0    [go, jurong, point, crazy, available, bugis, n...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, early, hor, u, c, already, say]\n",
            "4    [nah, dont, think, goes, usf, lives, around, t...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token lists are shorter at this stage and contain fewer non-informative words, setting a cleaner stage for future text transformations."
      ],
      "metadata": {
        "id": "vmsCLoZyDbwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming\n",
        "Stemming normalizes words by reducing them to their base form (e.g., running becomes run). This consolidates different forms of the same root word, effectively cutting the vocabulary size and smoothing out the text representation. As a result, the model can better understand the underlying concepts without being distracted by trivial variations in word forms."
      ],
      "metadata": {
        "id": "9SFflV2eDdvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Stem each token to reduce words to their base form\n",
        "stemmer = PorterStemmer()\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "print(\"\\n=== AFTER STEMMING ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxe_4M-IDc59",
        "outputId": "15bb414f-307a-4453-d86c-be4753681963"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER STEMMING ===\n",
            "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
            "1                         [ok, lar, joke, wif, u, oni]\n",
            "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
            "4    [nah, dont, think, goe, usf, live, around, tho...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After stemming, the token lists focus on root word forms, further simplifying the text and improving the model’s generalization ability."
      ],
      "metadata": {
        "id": "2-EO8Ig8DzFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Joining Tokens Back into a Single String\n",
        "While tokens are useful for manipulation, many machine-learning algorithms and vectorization techniques (e.g., TF-IDF) work best with raw text strings. Rejoining the tokens into a space-separated string restores a format compatible with these methods, allowing the dataset to move seamlessly into the feature extraction phase."
      ],
      "metadata": {
        "id": "M-wRrndfDz7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin tokens into a single string for feature extraction\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
        "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcQLE1maD1a7",
        "outputId": "6318abb3-4ad3-407e-c512-aaf86e9ae37b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
            "0    go jurong point crazi avail bugi n great world...\n",
            "1                                ok lar joke wif u oni\n",
            "2    free entri wkli comp win fa cup final tkt st m...\n",
            "3                  u dun say earli hor u c alreadi say\n",
            "4            nah dont think goe usf live around though\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction\n",
        "Feature extraction transforms preprocessed SMS messages into numerical vectors suitable for machine learning algorithms. Since models cannot directly process raw text data, they rely on numeric representations—such as counts or frequencies of words—to identify patterns that differentiate spam from ham.\n",
        "\n",
        "##Representing Text as Numerical Features\n",
        "A common way to represent text numerically is through a bag-of-words model. This technique constructs a vocabulary of unique terms from the dataset and represents each message as a vector of term counts. Each element in the vector corresponds to a term in the vocabulary, and its value indicates how often that term appears in the message.\n",
        "\n",
        "Using only unigrams (individual words) does not preserve the original word order; it treats each document as a collection of terms and their frequencies, independent of sequence.\n",
        "\n",
        "To introduce a limited sense of order, we also include bigrams, which are pairs of consecutive words. By incorporating bigrams, we capture some local ordering information.\n",
        "\n",
        "For example, the bigram free prize might help distinguish a spam message promising a reward from a simple statement containing the word free alone. However, beyond these small sequences, the global order of words and sentence structure remains largely lost. In other words, CountVectorizer does not preserve complete word order; it only captures localized patterns defined by the chosen ngram_range.\n",
        "\n",
        "##Using CountVectorizer for the Bag-of-Words Approach\n",
        "CountVectorizer from the scikit-learn library efficiently implements the bag-of-words approach. It converts a collection of documents into a matrix of term counts, where each row represents a message and each column corresponds to a term (unigram or bigram). Before transformation, CountVectorizer applies tokenization, builds a vocabulary, and then maps each document to a numeric vector.\n",
        "\n",
        "Key parameters for refining the feature set:\n",
        "\n",
        "- min_df=1: A term must appear in at least one document to be included. While this threshold is set to 1 here, higher values can be used in practice to exclude rare terms.\n",
        "- max_df=0.9: Terms that appear in more than 90% of the documents are excluded, removing overly common words that provide limited differentiation.\n",
        "- ngram_range=(1, 2): The feature matrix captures individual words and common word pairs by including unigrams and bigrams, potentially improving the model’s ability to detect spam patterns."
      ],
      "metadata": {
        "id": "g7ETqnapEbd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
        "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the message column\n",
        "X = vectorizer.fit_transform(df[\"message\"])\n",
        "\n",
        "# Labels (target variable)\n",
        "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0"
      ],
      "metadata": {
        "id": "4AFYsAMRE6Br"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step, X becomes a numerical feature matrix ready to be fed into a classifier, such as Naive Bayes."
      ],
      "metadata": {
        "id": "rllpMHtCE7-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How CountVectorizer Works\n",
        "CountVectorizer operates in three main stages:\n",
        "\n",
        "1. **Tokenization**: Splits the text into tokens based on the specified ngram_range. For ngram_range=(1, 2), it extracts both unigrams (like \"message\") and bigrams (like \"free prize\").\n",
        "2. **Building the Vocabulary**: Uses min_df and max_df to decide which terms to include. Terms that are too rare or common are filtered out, leaving a vocabulary that balances informative and distinctive terms.\n",
        "3. **Vectorization**: Transforms each document into a vector of term counts. Each vector entry corresponds to a term from the vocabulary, and its value represents how many times that term appears in the document.\n",
        "\n",
        "##Example with Unigrams\n",
        "Consider five documents:\n",
        "\n",
        "1. The free prize is waiting for you\n",
        "2. The spam message offers a free prize now\n",
        "3. The spam filter might detect this\n",
        "4. The important news says you won a free trip\n",
        "5. The message truly is important\n",
        "\n",
        "If we use ngram_range=(1, 1) (unigrams only) and min_df=1, max_df=0.9, the word The will be removed from unigram vocabulary by max_df=0.9 since it appears more than 90% in the documents, leaving the below unigram matrix:"
      ],
      "metadata": {
        "id": "qHsR1fqyE91-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example with Bigrams\n",
        "Using ngram_range=(1, 2), the final vocabulary includes all of the above unigrams and any valid bigrams containing those unigrams. For instance, free prize occurs in Documents 1 and 2. The resulting matrix provides additional context, helping the model differentiate messages more effectively:"
      ],
      "metadata": {
        "id": "3IMRc03QFPDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feature extraction process, using CountVectorizer, has transformed our text data into a resulting matrix provides a concise, numerical representation of each message, ready for training a classification model."
      ],
      "metadata": {
        "id": "1OEoFJSLFWTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluation (Spam Detection)\n",
        "##Training\n",
        "After preprocessing the text data and extracting meaningful features, we train a machine-learning model for spam detection. We use the Multinomial Naive Bayes classifier, which is well-suited for text classification tasks due to its probabilistic nature and ability to efficiently handle large, sparse feature sets.\n",
        "\n",
        "To streamline the entire process, we employ a Pipeline. A pipeline chains together the vectorization and modeling steps, ensuring that the same data transformation (in this case, CountVectorizer) is consistently applied before feeding the transformed data into the classifier. This approach simplifies both development and maintenance by encapsulating the feature extraction and model training into a single, unified workflow."
      ],
      "metadata": {
        "id": "gfQWrZT-MVu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build the pipeline by combining vectorization and classification\n",
        "pipeline = Pipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "vjjtUMSKMYdF"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the pipeline in place, we can easily integrate hyperparameter tuning to improve model performance. The objective is to find optimal parameter values for the classifier, ensuring that the model generalizes well and avoids overfitting.\n",
        "\n",
        "To achieve this, we use GridSearchCV. This method systematically searches through specified hyperparameter values to identify the configuration that produces the best performance. In the case of MultinomialNB, we focus on the alpha parameter, a smoothing factor that adjusts how the model handles unseen words and prevents probabilities from being zero. We can balance bias and variance by tuning alpha, ultimately improving the model’s robustness."
      ],
      "metadata": {
        "id": "yk5EbItQNcFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
        "}\n",
        "\n",
        "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"f1\"\n",
        ")\n",
        "\n",
        "# Fit the grid search on the full dataset\n",
        "grid_search.fit(df[\"message\"], y)\n",
        "\n",
        "# Extract the best model identified by the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best model parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHYFrs1zNcuK",
        "outputId": "8410c9dd-3ab6-48fa-a46b-7080b8381bdd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model parameters: {'classifier__alpha': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The combination of Pipeline and GridSearchCV ensures a clean, consistent workflow. First, CountVectorizer converts raw text into numeric features suitable for the classifier. Next, MultinomialNB applies its probabilistic principles to distinguish between spam and ham messages.\n",
        "\n",
        "Finally, by evaluating alpha values and leveraging cross-validation, we reliably select the best configuration based on the F1-score, a balanced metric for precision and recall."
      ],
      "metadata": {
        "id": "cHKlBkJkNehi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "PICTURE HERE"
      ],
      "metadata": {
        "id": "19KiLEypNhrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training and fine-tuning the spam detection model, assessing its performance on new, unseen SMS messages is critical. This evaluation helps verify how well the model generalizes to real-world data and highlights improvement areas. The evaluation mirrors the preprocessing and feature extraction steps applied during training, ensuring a consistent and fair assessment of the model’s true predictive capabilities.\n",
        "\n",
        "##Setting Up the Evaluation Messages\n",
        "We begin by providing a list of new SMS messages for evaluation. These messages represent the types of inputs the model might receive in real-world use, including promotional offers, routine communications, urgent alerts, reminders, and incentive-based spam."
      ],
      "metadata": {
        "id": "3bqthA-oNkF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example SMS messages for evaluation\n",
        "new_messages = [\n",
        "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
        "    \"Hey, are we still meeting up for lunch today?\",\n",
        "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
        "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
        "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
        "]"
      ],
      "metadata": {
        "id": "vg1ZG-XqNqyg"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing New Messages\n",
        "Before predicting with the trained model, we must preprocess the new messages using the same steps applied during training. Consistent preprocessing ensures that the model receives data in the same format it was trained on. The preprocess_message function converts each message to lowercase, removes non-alphabetic characters, tokenizes the text, removes stop words, and applies stemming."
      ],
      "metadata": {
        "id": "txfqtAeiNsGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Preprocess function that mirrors the training-time preprocessing\n",
        "def preprocess_message(message):\n",
        "    message = message.lower()\n",
        "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
        "    tokens = word_tokenize(message)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "r6BXeNAfNth_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we apply this function to each of the new messages:\n",
        "\n"
      ],
      "metadata": {
        "id": "hAYeu_3uNvBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and vectorize messages\n",
        "processed_messages = [preprocess_message(msg) for msg in new_messages]"
      ],
      "metadata": {
        "id": "8Z4s7w9cNw-P"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorizing the Processed Messages\n",
        "The model expects numerical input features. To achieve this, we apply the same vectorization method used during training. The CountVectorizer saved within the pipeline (best_model.named_steps[\"vectorizer\"]) transforms the preprocessed text into a numerical feature matrix."
      ],
      "metadata": {
        "id": "6ay59zNdN5so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform preprocessed messages into feature vectors\n",
        "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
      ],
      "metadata": {
        "id": "IhlonaosN8Is"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Making Predictions\n",
        "With the data properly preprocessed and vectorized, we feed the new messages into the trained MultinomialNB classifier (best_model.named_steps[\"classifier\"]). This classifier outputs both a predicted label (spam or not spam) and class probabilities, indicating the model’s confidence in its decision."
      ],
      "metadata": {
        "id": "ZwpHueiKN-Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict with the trained classifier\n",
        "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
        "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
      ],
      "metadata": {
        "id": "oXnPOdGoN-4r"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Displaying Predictions and Probabilities\n",
        "The next step is to present the evaluation results. For each message, we display:\n",
        "\n",
        "- The original text of the message.\n",
        "- The predicted label (Spam or Not-Spam).\n",
        "- The probability that the message is spam.\n",
        "- The probability that the message is not spam.\n",
        "\n",
        "This output provides insight into the model’s reasoning and confidence levels, making it easier to understand and trust the predictions."
      ],
      "metadata": {
        "id": "519oTtJuOB-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display predictions and probabilities for each evaluated message\n",
        "for i, msg in enumerate(new_messages):\n",
        "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
        "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
        "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
        "\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
        "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMF2PQbcOFDl",
        "outputId": "2db03fa0-586c-462d-b77a-3d4caa2313ce"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n",
            "Message: Hey, are we still meeting up for lunch today?\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
            "Prediction: Spam\n",
            "Spam Probability: 0.96\n",
            "Not-Spam Probability: 0.04\n",
            "--------------------------------------------------\n",
            "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A representative output might look like this:\n",
        "\n",
        "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
        "Prediction: Spam\n",
        "Spam Probability: 1.00\n",
        "Not-Spam Probability: 0.00\n",
        "--------------------------------------------------\n",
        "Message: Hey, are we still meeting up for lunch today?\n",
        "Prediction: Not-Spam\n",
        "Spam Probability: 0.00\n",
        "Not-Spam Probability: 1.00\n",
        "--------------------------------------------------\n",
        "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
        "Prediction: Spam\n",
        "Spam Probability: 0.94\n",
        "Not-Spam Probability: 0.06\n",
        "--------------------------------------------------\n",
        "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
        "Prediction: Not-Spam\n",
        "Spam Probability: 0.00\n",
        "Not-Spam Probability: 1.00\n",
        "--------------------------------------------------\n",
        "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
        "Prediction: Spam\n",
        "Spam Probability: 1.00\n",
        "Not-Spam Probability: 0.00\n",
        "--------------------------------------------------"
      ],
      "metadata": {
        "id": "z3waabV6OGjz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZDRDuOkOHVG"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results show that the model can differentiate between benign messages and a range of spam content, providing both a categorical decision and the underlying probability estimates."
      ],
      "metadata": {
        "id": "IfxE2wsGOPCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using joblib for Saving Models\n",
        "After confirming satisfactory performance, preserving the trained model to be reused later is often necessary. By saving the model to a file, users can avoid the computational expense of retraining it from scratch each time. This is especially helpful in production environments where quick predictions are required.\n",
        "\n",
        "joblib is a Python library designed to efficiently serialize and deserialize Python objects, particularly those containing large arrays such as NumPy arrays or scikit-learn models. Serialization converts an in-memory object into a format that can be stored on disk or transmitted across networks. Deserialization involves converting the stored representation back into an in-memory object with the exact same state it had when saved.\n",
        "\n",
        "joblib works by leveraging optimized binary file formats that compress and split objects, if necessary, to handle large datasets or complex models. When a model, such as a scikit-learn pipeline, is saved with joblib, it stores the entire model state including learned parameters and configurations. Later, when the model is reloaded, it will immediately be ready to make predictions as if it had just been trained.\n",
        "\n",
        "By doing so, joblib helps streamline the deployment process. Instead of retraining the model every time the application restarts, developers and operations teams can load the saved model into memory and start making predictions. This reduces both computational overhead and startup latency."
      ],
      "metadata": {
        "id": "uTYipRm0OQFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file for future use\n",
        "model_filename = 'spam_detection_model.joblib'\n",
        "joblib.dump(best_model, model_filename)\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhNwCxc3OTLX",
        "outputId": "636aafd8-aef9-4aaf-cf33-adb2cfb941c9"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to spam_detection_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, best_model likely refers to a finalized and tested pipeline or classifier. The file spam_detection_model.joblib will contain all the necessary information to predict new data. To reuse the model later, load it back into the environment:"
      ],
      "metadata": {
        "id": "nt28BtRfOUPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = joblib.load(model_filename)\n",
        "predictions = loaded_model.predict(new_messages)"
      ],
      "metadata": {
        "id": "4zeRX-_xOVUN"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation (Spam Detection)\n"
      ],
      "metadata": {
        "id": "lKa4yU-TO4P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import json\n",
        "\n",
        "# # Define the URL of the API endpoint\n",
        "# url = \"http://localhost:8000/api/upload\"\n",
        "\n",
        "# # Path to the model file you want to upload\n",
        "# model_file_path = \"spam_detection_model.joblib\"\n",
        "\n",
        "# # Open the file in binary mode and send the POST request\n",
        "# with open(model_file_path, \"rb\") as model_file:\n",
        "#     files = {\"model\": model_file}\n",
        "#     response = requests.post(url, files=files)\n",
        "\n",
        "# # Pretty print the response from the server\n",
        "# print(json.dumps(response.json(), indent=4))"
      ],
      "metadata": {
        "id": "vdcx2I29O6I3"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}